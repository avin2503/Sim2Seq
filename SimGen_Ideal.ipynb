{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88667e68-ac15-4c74-9788-e9d42dafee4e",
   "metadata": {},
   "source": [
    "### SimGen_Ideal signal generator functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676e7292-96f3-44a8-941c-65351c4d5bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import psutil\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from Bio import SeqIO\n",
    "from scipy.signal import resample\n",
    "from datetime import datetime\n",
    "import concurrent.futures\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def normalize_signal(signal):\n",
    "    mean = 63.63331112435138 # can be changed if needed, depending on dataset\n",
    "    std = 10.705338783244988 # can be changed if needed, depending on dataset\n",
    "    std = std if std != 0 else 1\n",
    "    normalized = (signal - mean) / std\n",
    "    print(\"mean is\", mean, \"and std is\", std)\n",
    "\n",
    "    return normalized\n",
    "    \n",
    "def moving_6mer_Substrings(string):\n",
    "    return [string[i:i+6] for i in range(len(string) - 5)]\n",
    "\n",
    "def predict_DNA_6mer_5_3_with_sampling(template, lut, lambda_time, sampling_rate, I_max=180):\n",
    "    template = template[::-1] \n",
    "    kmers = moving_6mer_Substrings(template)\n",
    "    N = len(kmers)\n",
    "\n",
    "    valid_kmers = [k for k in kmers if k in lut]\n",
    "    params = np.array([list(lut[k].values()) for k in valid_kmers]) \n",
    "\n",
    "    pre_mean, pre_std, post_mean, post_std = params.T\n",
    "\n",
    "    step_times = np.ones(len(valid_kmers)) * lambda_time \n",
    "    num_samples = (step_times * sampling_rate).astype(int)\n",
    "\n",
    "    sampled_signals = []\n",
    "    sampled_times = []\n",
    "    current_time = 0.0\n",
    "    \n",
    "    for i in range(len(valid_kmers)):\n",
    "        ns = num_samples[i]\n",
    "        if ns == 0:\n",
    "            continue\n",
    "        \n",
    "        pre = np.random.normal(pre_mean[i] * I_max, pre_std[i] * I_max, ns)\n",
    "        post = np.random.normal(post_mean[i] * I_max, post_std[i] * I_max, ns)\n",
    "        \n",
    "        step_time = step_times[i]\n",
    "        \n",
    "        t_pre = np.linspace(current_time, current_time + step_time, ns)\n",
    "        sampled_signals.extend(pre)\n",
    "        sampled_times.extend(t_pre)\n",
    "        current_time += step_time\n",
    "       \n",
    "        t_post = np.linspace(current_time, current_time + step_time, ns)\n",
    "        sampled_signals.extend(post)\n",
    "        sampled_times.extend(t_post)\n",
    "        current_time += step_time\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        \"time\": sampled_times,\n",
    "        \"current\": sampled_signals\n",
    "    })\n",
    "\n",
    "def extract_random_sequences(reference_genome_file, num_sequences, seq_length):\n",
    "    genome = \"\".join(str(record.seq) for record in SeqIO.parse(reference_genome_file, \"fasta\"))\n",
    "    genome_len = len(genome)\n",
    "    sequences = set()\n",
    "    \n",
    "    while len(sequences) < num_sequences:\n",
    "        start = random.randint(0, genome_len - seq_length)\n",
    "        seq = genome[start:start + seq_length]\n",
    "        sequences.add(seq)\n",
    "    \n",
    "    return list(sequences)\n",
    "\n",
    "def encode_dna_sequence(seq):\n",
    "    mapping = {'A': 1, 'C': 2, 'G': 3, 'T': 4} # if reference genome includes lower case, those can be added here \n",
    "    return [mapping.get(base.upper(), 0) for base in seq]\n",
    "\n",
    "def generate_random_signals(reference_genome_file, num_reads, seq_length):\n",
    "    sequences = extract_random_sequences(reference_genome_file, num_reads, seq_length)\n",
    "    test_size = max(1, int(0.05 * len(sequences)))\n",
    "    test_indices = np.random.choice(len(sequences), size=test_size, replace=False)\n",
    "    test_sequences = [sequences[i] for i in test_indices]\n",
    "    train_sequences = [sequences[i] for i in range(len(sequences)) if i not in test_indices]\n",
    "    return train_sequences, test_sequences\n",
    "\n",
    "def chunk_signal_optimized(n_chunks, signal, chunksize, overlap):\n",
    "    step = chunksize - overlap\n",
    "    result = np.zeros((n_chunks, chunksize))\n",
    "    \n",
    "    for i in range(n_chunks):\n",
    "        start = i * step\n",
    "        result[i] = signal[start:start + chunksize]\n",
    "        \n",
    "    return result\n",
    "\n",
    "def align_references(bases_per_point,dna_sequence, signal_length, chunksize, overlap): \n",
    "    \n",
    "    dna_encoded = encode_dna_sequence(dna_sequence) \n",
    "    step = chunksize - overlap\n",
    "\n",
    "    raw_references = []\n",
    "    reference_lengths = []\n",
    "    \n",
    "    for i in range(0, signal_length - chunksize + 1, step):\n",
    "        start = int(i * bases_per_point)\n",
    "        end = int((i + chunksize) * bases_per_point)\n",
    "\n",
    "        chunk_ref = dna_encoded[start:end]\n",
    "        \n",
    "        reference_lengths.append(len(chunk_ref))\n",
    "        raw_references.append(chunk_ref[::-1])\n",
    "\n",
    "    max_len = max(reference_lengths)\n",
    "    padded_references = [np.pad(ref, (0, max_len - len(ref)), 'constant', constant_values=0) for ref in raw_references] \n",
    "\n",
    "    return np.array(padded_references, dtype=np.int8), np.array(reference_lengths, dtype=np.int64)\n",
    "\n",
    "def process_batch_in_parallel(batch_seqs, lut, lambda_time, sampling_rate, output_dir, batch_idx):\n",
    "    all_chunks, all_refs, all_ref_lens = [None] * len(batch_seqs), [None] * len(batch_seqs), [None] * len(batch_seqs)\n",
    "\n",
    "    for seq_i, seq in enumerate(batch_seqs):\n",
    "        signal_df = predict_DNA_6mer_5_3_with_sampling(seq, lut, lambda_time, sampling_rate)\n",
    "        signal = signal_df['current'].values\n",
    "        sig_len = len(signal)\n",
    "        dna_len = len(seq)\n",
    "        chunksize, overlap = 10000, 500 # chunk size and overlap values changed here\n",
    "        bases_per_point = 0.05\n",
    "        \n",
    "        step = chunksize - overlap\n",
    "        n_chunks = (sig_len - overlap) // step\n",
    "        retained_signal_length = n_chunks * step + overlap\n",
    "        trimmed_signal = signal[:retained_signal_length]\n",
    "        normalized_signal = normalize_signal(trimmed_signal)\n",
    "        chunks = chunk_signal_optimized(n_chunks, normalized_signal, chunksize, overlap)\n",
    "        trimmed_seq_length = dna_len-int(retained_signal_length * bases_per_point)#+1\n",
    "        trimmed_seq = seq[trimmed_seq_length:]\n",
    "        refs, ref_lens = align_references(bases_per_point, trimmed_seq, len(trimmed_signal), chunksize, overlap)\n",
    "\n",
    "        all_chunks[seq_i] = chunks\n",
    "        all_ref_lens[seq_i] = ref_lens\n",
    "        all_refs[seq_i] = refs\n",
    "\n",
    "    all_refs = [ref[::-1] for ref in all_refs]  \n",
    "    all_ref_lens = [lens[::-1] for lens in all_ref_lens]\n",
    "    \n",
    "    max_ref_len = int(max(np.concatenate(all_ref_lens)))\n",
    "    padded = []\n",
    "    for ref in all_refs:\n",
    "        padded_ref = np.array([\n",
    "            np.pad(chunk, (0, max_ref_len - len(chunk)), 'constant', constant_values=0) \n",
    "            for chunk in ref\n",
    "        ], dtype=np.uint8)\n",
    "        padded.append(padded_ref)\n",
    "    \n",
    "    output_file = os.path.join(output_dir, f\"batch_{batch_idx}.npz\")\n",
    "    np.savez_compressed(output_file,\n",
    "                        chunks=np.concatenate(all_chunks),\n",
    "                        references=np.concatenate(padded),\n",
    "                        reference_lengths=np.concatenate(all_ref_lens),\n",
    "                        max_ref_len=max_ref_len)\n",
    "    del all_chunks, all_refs, all_ref_lens\n",
    "    gc.collect()\n",
    "    return output_file\n",
    "\n",
    "def process_and_save_batches(sequences, lut, output_dir, prefix, lambda_time, sampling_rate, batch_size):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    saved_files = []\n",
    "    global_max_ref_len = 0\n",
    "\n",
    "    with concurrent.futures.ProcessPoolExecutor(max_workers=4) as executor: # number of workers can be changed here \n",
    "        futures = {}\n",
    "        for batch_idx, start in enumerate(range(0, len(sequences), batch_size)):\n",
    "            end = min(start + batch_size, len(sequences))\n",
    "            batch_seqs = sequences[start:end]\n",
    "            future = executor.submit(\n",
    "                process_batch_in_parallel,\n",
    "                batch_seqs, lut, lambda_time, sampling_rate, output_dir, batch_idx\n",
    "            )\n",
    "            futures[future] = batch_idx\n",
    "\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            batch_idx = futures[future]\n",
    "            try:\n",
    "                output_file = future.result()\n",
    "                saved_files.append(output_file)\n",
    "\n",
    "                with np.load(output_file) as data:\n",
    "                    global_max_ref_len = max(global_max_ref_len, int(data[\"max_ref_len\"]))\n",
    "                print_memory_usage(f\"After batch {batch_idx}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed batch {batch_idx}: {e}\")\n",
    "            gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce13e51a-af54-4fd2-abf6-ccdef7dba6b3",
   "metadata": {},
   "source": [
    "### Example usage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be43abf-7caf-453a-8f7a-a48bed62e820",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_genome = '...' # in .fna format preferably\n",
    "LUT_6mer = pd.read_csv('...', encoding='utf-8') # add kmer model \n",
    "lut = LUT_6mer.set_index(\"kmer_pull_3_5\")[[\"pre_mean\", \"pre_std\", \"post_mean\", \"post_std\"]].to_dict(\"index\") # change parameter names if needed\n",
    "\n",
    "\n",
    "train_seqs, test_seqs = generate_random_signals(reference_genome_file=reference_genome, num_reads=...,seq_length=...) # add values here\n",
    "\n",
    "output_dir_train = \"...\" # output training files directory \n",
    "output_dir_test = \"...\" # validation set folder within the training output folder\n",
    "\n",
    "\n",
    "process_and_save_batches(\n",
    "    sequences=train_seqs,\n",
    "    lut=lut,\n",
    "    output_dir=output_dir_train,\n",
    "    prefix=\"train\",\n",
    "    lambda_time=..., # add dwell time, preferably similar to 0.002 s \n",
    "    sampling_rate=5000, # change if needed \n",
    "    batch_size=500 # change if needed \n",
    ")\n",
    "\n",
    "process_and_save_batches(\n",
    "    sequences=test_seqs,\n",
    "    lut=lut,\n",
    "    output_dir=output_dir_test,\n",
    "    prefix=\"test\",\n",
    "    lambda_time=..., # add same dwell time as for training data\n",
    "    sampling_rate=5000,\n",
    "    batch_size=500\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (run_code)",
   "language": "python",
   "name": "run_code"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
