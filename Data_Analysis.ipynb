{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6db918fb-7300-4153-8e01-ae83e8ed6ae3",
   "metadata": {},
   "source": [
    "# Data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bace1e68-5eaa-4999-a61e-7cc431cda694",
   "metadata": {},
   "source": [
    "This notebook contains code to detect the presence of the GAAGT seed motif in basecalled sequences, compute the mean accuracy of trained models by aligning basecalled reads to the ground truth reference sequence, and calculate the DTW (Dynamic Time Warping) distance for aligning experimental signals to reconstructed reference signals as an indication of read quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edeecba3-92ae-4765-9c23-cfac5c0d3ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from Bio import SeqIO\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from Bio import pairwise2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from fastdtw import fastdtw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a96d1d-aa7e-488b-8b8c-bb971b582636",
   "metadata": {},
   "source": [
    "## Checking for GAAGT seed within basecalled sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f906ef-60fd-4762-bdc4-dd64f56e0913",
   "metadata": {},
   "outputs": [],
   "source": [
    "fastq_file = \"path/to/fastq\"  \n",
    "output_csv = \"path/to/output.csv\"\n",
    "\n",
    "seed = \"GAAGT\"\n",
    "\n",
    "results = []\n",
    "total_reads = 0\n",
    "seed_hits = 0\n",
    "\n",
    "for record in SeqIO.parse(fastq_file, \"fastq\"):\n",
    "    seq = str(record.seq)\n",
    "    mid_index = len(seq) // 2 # checks in second half of the sequence \n",
    "    second_half = seq[mid_index:]\n",
    "    has_seed = seed in second_half\n",
    "\n",
    "    results.append({\n",
    "        \"read_id\": record.id,\n",
    "        \"sequence\": seq,\n",
    "        \"contains_seed_in_second_half\": has_seed\n",
    "    })\n",
    "\n",
    "    total_reads += 1\n",
    "    if has_seed:\n",
    "        seed_hits += 1\n",
    "\n",
    "with open(output_csv, mode=\"w\", newline=\"\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=[\"read_id\", \"sequence\", \"contains_seed_in_second_half\"])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(results)\n",
    "    \n",
    "print(f\"Total reads: {total_reads}\")\n",
    "print(f\"Reads with '{seed}' in second half: {seed_hits} ({(seed_hits / total_reads) * 100:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8dd9b2-27c9-4902-8da4-ed17cfe99ad1",
   "metadata": {},
   "source": [
    "## Calculate accuracy for experimental data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e59d0c-9b22-4088-a528-b85f21a554c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_cigar = r'(\\d+)([=IDX])'\n",
    "\n",
    "def compute_biopython_alignment(seq, ref):\n",
    "    \"\"\"\n",
    "    Perform global alignment and convert to pseudo-CIGAR format.\n",
    "    \"\"\"\n",
    "    alignments = pairwise2.align.globalms(seq, ref, 2, -3, -5, -2) # parameter values can be changed here \n",
    "    best = alignments[0]\n",
    "    aligned_seq, aligned_ref = best.seqA, best.seqB\n",
    "\n",
    "    operations = []\n",
    "    for s, r in zip(aligned_seq, aligned_ref):\n",
    "        if s == '-' and r != '-':\n",
    "            operations.append('D')\n",
    "        elif r == '-' and s != '-':\n",
    "            operations.append('I')\n",
    "        elif s == r:\n",
    "            operations.append('=')\n",
    "        else:\n",
    "            operations.append('X')\n",
    "\n",
    "    cigar = ''\n",
    "    count = 1\n",
    "    for i in range(1, len(operations)):\n",
    "        if operations[i] == operations[i - 1]:\n",
    "            count += 1\n",
    "        else:\n",
    "            cigar += f\"{count}{operations[i - 1]}\"\n",
    "            count = 1\n",
    "    cigar += f\"{count}{operations[-1]}\"\n",
    "    return cigar\n",
    "\n",
    "def accuracy_ignore_deletions(ref, seq):\n",
    "    \"\"\"\n",
    "    Compute accuracy as: match / (match + mismatch + insertion),\n",
    "    ignoring deletions.\n",
    "    \"\"\"\n",
    "    cigar = compute_biopython_alignment(seq, ref)\n",
    "    counts = defaultdict(int)\n",
    "\n",
    "    for count, op in re.findall(split_cigar, cigar):\n",
    "        counts[op] += int(count)\n",
    "\n",
    "    total = counts['='] + counts['X'] + counts['I']\n",
    "    acc = counts['='] / total * 100 if total > 0 else 0.0\n",
    "    return acc, counts\n",
    "\n",
    "\n",
    "fastq_path = \"path/to/basecalled_reads.fastq\"  # update this path\n",
    "basecalled_dict = {}\n",
    "\n",
    "for record in SeqIO.parse(fastq_path, \"fastq\"):\n",
    "    basecalled_dict[record.id] = str(record.seq)\n",
    "\n",
    "reference_sequence = \"CCGATGCTGGCTACATCTTAGGCTATCACTCTCACCTGCGATTATATGGTCCGTGCACTCTGAAGTCATT\" # change this if needed \n",
    "\n",
    "accuracies = []\n",
    "raw_totals = defaultdict(int)\n",
    "\n",
    "for read_id, called_seq in basecalled_dict.items():\n",
    "    acc, counts = accuracy_ignore_deletions(reference_sequence, called_seq)\n",
    "    accuracies.append(acc)\n",
    "    for k in counts:\n",
    "        raw_totals[k] += counts[k]\n",
    "\n",
    "if accuracies:\n",
    "    mean_accuracy = np.mean(accuracies)\n",
    "    print(f\"Mean accuracy (ignoring deletions): {mean_accuracy:.2f}%\")\n",
    "    print(f\"Evaluated {len(accuracies)} reads.\")\n",
    "    print(\"Alignment stats:\")\n",
    "    print(f\"  Matches (=):     {raw_totals['=']}\")\n",
    "    print(f\"  Mismatches (X):  {raw_totals['X']}\")\n",
    "    print(f\"  Deletions (D):   {raw_totals['D']}\")\n",
    "    print(f\"  Insertions (I):  {raw_totals['I']}\")\n",
    "else:\n",
    "    print(\"No reads found to evaluate.\")\n",
    "\n",
    "\n",
    "info_df = pd.read_csv(\"path/to/mapping.csv\")  # update this path --> this comes from .csv to .pod5 conversion script (mapping_df)\n",
    "\n",
    "length_errors = []\n",
    "matched_lengths = 0\n",
    "missing_from_basecalls = 0\n",
    "\n",
    "for _, row in info_df.iterrows():\n",
    "    read_id = row['read_id']\n",
    "    predicted_len = row['predicted length']\n",
    "\n",
    "    if read_id in basecalled_dict:\n",
    "        actual_len = len(basecalled_dict[read_id])\n",
    "        length_errors.append(actual_len - predicted_len)\n",
    "        matched_lengths += 1\n",
    "    else:\n",
    "        missing_from_basecalls += 1\n",
    "\n",
    "if length_errors:\n",
    "    mae = np.mean(np.abs(length_errors))\n",
    "    mse = np.mean(np.square(length_errors))\n",
    "    mean_error = np.mean(length_errors)\n",
    "\n",
    "    print(\"\\nBasecalled length vs predicted length:\")\n",
    "    print(f\"Matched reads: {matched_lengths}\")\n",
    "    print(f\"Missing from basecalls: {missing_from_basecalls}\")\n",
    "    print(f\"Mean error: {mean_error:.2f} bases\")\n",
    "    print(f\"MAE: {mae:.2f} bases\")\n",
    "    print(f\"MSE: {mse:.2f} bases\")\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.hist(length_errors, bins=50, color='steelblue', edgecolor='black')\n",
    "    plt.title(\"Length Prediction Error Distribution\")\n",
    "    plt.xlabel(\"Error (Basecalled - Predicted)\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No matching read IDs found between basecalls and predictions.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f48492-dd95-44c1-93ca-f1beffb28422",
   "metadata": {},
   "source": [
    "## DTW analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b020d9-18b4-42d9-9487-f7cd17e26187",
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_6mer_Substrings(string):\n",
    "    return [string[i:i+6] for i in range(len(string) - 5)]\n",
    "\n",
    "# change this to different signal generation if needed\n",
    "def predict_DNA_6mer_5_3_with_sampling(template, lut, lambda_time, sampling_rate, I_max=180): \n",
    "    template = template[::-1]\n",
    "    kmers = moving_6mer_Substrings(template)\n",
    "    N = len(kmers)\n",
    "\n",
    "    valid_kmers = [k for k in kmers if k in lut]\n",
    "    params = np.array([list(lut[k].values()) for k in valid_kmers]) \n",
    "\n",
    "    pre_mean, pre_std, post_mean, post_std = params.T\n",
    "\n",
    "    step_times = np.ones(len(valid_kmers)) * lambda_time \n",
    "    num_samples = (step_times * sampling_rate).astype(int)\n",
    "\n",
    "    sampled_signals = []\n",
    "    sampled_times = []\n",
    "    current_time = 0.0\n",
    "    \n",
    "    for i in range(len(valid_kmers)):\n",
    "        ns = num_samples[i]\n",
    "        if ns == 0:\n",
    "            continue\n",
    "        \n",
    "        pre = np.random.normal(pre_mean[i] * I_max, pre_std[i] * I_max, ns)\n",
    "        post = np.random.normal(post_mean[i] * I_max, post_std[i] * I_max, ns)\n",
    "        \n",
    "        step_time = step_times[i]\n",
    "        \n",
    "        t_pre = np.linspace(current_time, current_time + step_time, ns)\n",
    "        sampled_signals.extend(pre)\n",
    "        sampled_times.extend(t_pre)\n",
    "        current_time += step_time\n",
    "       \n",
    "        t_post = np.linspace(current_time, current_time + step_time, ns)\n",
    "        sampled_signals.extend(post)\n",
    "        sampled_times.extend(t_post)\n",
    "        current_time += step_time\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        \"time\": sampled_times,\n",
    "        \"current\": sampled_signals\n",
    "    })\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6294c5a-5129-4cb5-9d96-73330e544fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference = 'TTACTGAAGTCTCACGTGCCTGGTATATTAGCGTCCACTCTCACTATCGGATTCTACATCGGTCGTAGCC' # change if needed\n",
    "reference_genome = 'path/to/reference_genome.fna' # update path \n",
    "LUT_6mer = pd.read_csv('path/to/kmer_model.csv', encoding='utf-8') # update path \n",
    "lut = LUT_6mer.set_index(\"kmer_pull_3_5\")[[\"pre_mean\", \"pre_std\", \"post_mean\", \"post_std\"]].to_dict(\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3eb9cb-ee20-4853-a9b9-baa883aba2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_signal = predict_DNA_6mer_5_3_with_sampling(reference, lut, 0.002, 5000)['current'].values # change values if needed \n",
    "if reference_signal is None:\n",
    "    raise ValueError(\"No reference signal found or generated.\")\n",
    "\n",
    "folder_path = \"path/to/resampled_signals\"\n",
    "signal_files = [f for f in os.listdir(folder_path) if f.startswith(\"JS\") and f.endswith(\".csv\")]\n",
    "\n",
    "results = []\n",
    "\n",
    "for file in signal_files:\n",
    "    file_path = os.path.join(folder_path, file)\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    if \"RelativeCurrent\" not in df.columns:\n",
    "        continue\n",
    "\n",
    "    test_signal = df[\"RelativeCurrent\"].values.astype(float)\n",
    "    dtw_distance, _ = fastdtw(reference_signal, test_signal, dist=lambda x, y: abs(x - y))\n",
    "    results.append((file, dtw_distance))\n",
    "\n",
    "results_df = pd.DataFrame(results, columns=[\"Filename\", \"DTW_Distance\"])\n",
    "\n",
    "dtw_threshold = results_df[\"DTW_Distance\"].quantile(0.25)\n",
    "results_df[\"Pass\"] = results_df[\"DTW_Distance\"] <= dtw_threshold\n",
    "\n",
    "print(f\"DTW distance threshold (25th percentile): {dtw_threshold:.2f}\")\n",
    "print(f\"{results_df['Pass'].sum()} / {len(results_df)} reads passed DTW threshold.\")\n",
    "\n",
    "results_df.to_csv(\"path/to/output/signal_quality_dtw_top25.csv\", index=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(results_df[\"DTW_Distance\"], bins=30, color='lightcoral', edgecolor='black')\n",
    "plt.axvline(dtw_threshold, color='blue', linestyle='--', label=f'Threshold (25th percentile) = {dtw_threshold:.2f}')\n",
    "plt.xlabel(\"DTW Distance\")\n",
    "plt.ylabel(\"Number of Signals\")\n",
    "plt.title(\"Distribution of DTW Distance Scores\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "best_match = results_df.loc[results_df[\"DTW_Distance\"].idxmin()]\n",
    "worst_match = results_df.loc[results_df[\"DTW_Distance\"].idxmax()]\n",
    "\n",
    "print(f\"Best match: {best_match['Filename']} | DTW Distance: {best_match['DTW_Distance']:.2f}\")\n",
    "print(f\"Worst match: {worst_match['Filename']} | DTW Distance: {worst_match['DTW_Distance']:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (run_code)",
   "language": "python",
   "name": "run_code"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
